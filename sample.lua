-- Thor Jonsson
-- This program provides a table of functions to sample an NLP neural network

local althingismadur = {}

function load()
	dl = require 'dataload/'
	rnn = require 'rnn'
	local path = './char_results/'
	local t7file = 'althingi_lstm_seqlen100hiddensize2x200trainsize2to10.t7'
    return torch.load(path..t7file)
end

althingismadur.DNN = load()
-- Okay almost there but definitely something off with our lstm! :/
function althingismadur.speak_out(table)
    ivocab = althingismadur.DNN.testset.icharvocab
    sequence = ""
    val = ""
    for i,val in ipairs(table) do
        print(val)
        next_char = ivocab[torch.floor(val)]
        if next_char ~= nil then
            sequence = sequence .. ivocab[torch.floor(val)]
            print(sequence)
        end
    end
    print(sequence)
end


function althingismadur.sample()
-- subiter takes in two arguments, i.e. batchsize and epochsize
-- inputs : seqlen x batchsize [x inputsize] TODO I get batchsize x seqlen
-- targets : seqlen x batchsize [x inputsize] 64 x 32 x 15 
    model = althingismadur.DNN.model
    opt = althingismadur.DNN.opt
    targetmodule = althingismadur.DNN.targetmodule
    testset = althingismadur.DNN.testset
    for i, inputs, targets in testset:subiter(1, opt.testsize) do
        -- forward the target through criterion
        -- Double cast is needed because of convolution
        while i < 100 do
         targets = targetmodule:forward(targets) -- intTensor with dimension 32 x 32 
         -- forward the model to obtain an output batch
         output = model:forward(inputs)
         output_table = torch.totable(output[1])
         althingismadur.speak_out(output_table[1])
        end
    end
end

return althingismadur







---- Loads the checkpoint
--function load()
--    model = xp:model()
--    print(torch.type(model))
--  
--end
--
---- Asks user to give input to seed
----function ask4input()
----  io.write('Give the computer an input to correct: \n')
----  io.flush()
----  local answer = io.read()
----  -- split the answer into words and put it in a table
----  return split_string(answer)
----end
--
----[[
----This file samples characters from a trained model
----Code
----[[
--This file samples characters from a trained model
--Code is based on implementation in Andrej Karpathy's https://github.com/karpathy/char-rnn
--which was in turn based on implementation in 
--https://github.com/oxford-cs-ml-2015/practical6
--]]--
--
--require 'nn'
--require 'rnn'
--require 'sys'
--require 'os'
--require 'torch'
--require 'paths'
--require 'nngraph'
--
--cmd = torch.CmdLine()
--cmd:text()
--cmd:text('Sample from a character-level language model')
--cmd:text()
--cmd:text('Options')
--cmd:argument('-xplog','experiment log to use for sampling (generated by train.lua)')
--cmd:option('-back', 'cpu', 'cpu|cuda|cl')
--cmd:option('-len', 2000, 'number of characters to sample')
--cmd:option('-temp', 1, 'temperature of sampling')
--cmd:option('-device', 1, 'which GPU device to use')
--cmd:text()
--
--local opt = cmd:parse(arg)
--
--local xplog = torch.load("./blstm_2-high_2-lstm_2/fortran:1459010874:1.dat")
--local model = xplog:model()
--local net = model.module -- nn.Serial(module) -> module
--local vocab = xplog.vocab
--local ivocab = {}
--for char, i in pairs(vocab) do
--  ivocab[i] = char
--end
--
--print('net', net)
--
--net:float()
--
--local q
--local function sampleSoftMax(log_q, t)
--  -- eg see https://en.wikipedia.org/wiki/Softmax_function, section 'Reinforcement learning'
--  q = q or log_q.new()
--  q:exp(log_q)
--  q:mul(1/t)
--  local sum_q = q:sum()
--  q:div(sum_q)
--  local sample = torch.multinomial(q, 1)
--  return sample
--end
--
---- seed with '\n' for now. a bit too much prior knowledge introduced by doing this, but
---- gets it working for now....
--local newLine = '\n'
--local prevChar = vocab[newLine]
--
--net:evaluate()
--net:remember('both')
--
--local sample = {}
--local input = torch.LongTensor(1,1)
--for i=1,opt.len do
--  input[{1,1}] = prevChar
--  local output = net:forward(input)[1]
--  local thisChar = sampleSoftMax(output[1], opt.temp)
--  local sampleChar = ivocab[thisChar[1]]
--  table.insert(sample, sampleChar)
--  prevChar = thisChar[1]
--end
--print(table.concat(sample,''))
--

